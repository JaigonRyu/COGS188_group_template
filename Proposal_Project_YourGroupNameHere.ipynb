{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",

    "- Jacob Hanshaw\n",
    "- Jonathan Ito\n",
    "- Hiroki Ito\n",
    "- Rebecca Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our project focuses on developing a reinforcement learning (RL) agent that can play Super Mario Bros using the Stable-Retro library, which turns classic video games into Gymnasium environments. The objective is to optimize performance in navigating levels, avoiding obstacles, and maximizing the game score. The agent will receive pixel-based observations and game state data, such as Mario's position, enemy locations, and score to make policy-based decisions using Monte Carlo and Q-learning. The agent will interact with the environment to iteratively adjust its policy based on rewards, which will be computed through level progression, survival time, and game score. Performance will be measured based on key metrics such as the averages of distance traveled, levels completed, and rewards achieved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorign the gaming world using AI is a subject that has been investigated for years. With the expansion of AI in gaming, there have been numerous uses for AI in gaming, and in other industries. Though classic games such as Super Mario Bros may seem intutive to play, or simple enought with only 6 inputs, there are over 16 different inputs, and different varibales like obstacles, enemies, power up, lives, and timers. By showing that AI is capable of performing so many differnt tasks at once is a great way to show that AI is a reliable method to navigate dynamic enviroemtns. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are numerous methods, but reinforcement learning is by far the most effecient and most explored method. In that regard, there are many instances of AI being used to complete variouos games. In these papers, we see that regardless of the game, pokemon <a name=\"Kalose\"></a>[<sup>[2]</sup>](#Kalose) , mario<a name=\"Liao\"></a>[<sup>[3]</sup>](#Liao), or street fighter <a name=\"Huertas\"></a>[<sup>[1]</sup>](#Huertas), Q-learning and reinforcemnet learning are empahsised. Using these papers as a benchmark on our own agent, we will also utilze similar model adn a similar approach. Though we are taking inspiration from these papers, however, we will be making our own agent and conducting our own tests. We will utilize thier result, however, in regardss to the mario paper, to compare our model with outside resources.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are attempting to utilize reinforcement learning to create a model free AI agent that is capable of completing a given level in Super Mario Bros. In the system, we will utilize Q-learning and measure its learning and success rates. We will also utilize the success of the agent by attempting various other similar and vastly different stages within the game, to measure how quickly it is able to adjust and learn to these new environments. \n",
    "Success will be measured if the number of iterations required to clear a new level is vastly less than the number of attempts required in its initial training in the first level. \n",
    "\n",
    "If time allows us to do so, we will train the initial agent on various levels to test how the learning and adaptability of the agent changes based on the initial level's difficulty and characteristics. (ie. if it is trained on a underwater level, how does it impact the training?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For our data set, we will be using a fork of the old gym-retro called stable-retro. Here, the creators add new retro games that users can use for different projects. \n",
    "\n",
    "For our objective, we will use Super Mario 2. The dataset size will depend on the number of frames we store from the gameplay. We will likely only play for a couple of minutes at a time, dealing with fewer frames than full playthroughs. The individual frames will consist of RGB images and game state information. \n",
    "\n",
    "Each observation will consist of the pixel data from the game screen, the discrete action space, and the rewards or metrics we choose. We will also take into account the environment states. \n",
    "\n",
    "Some of our critical variables will include the game frame being stored as an image. The agent actions for doing nothing, an action, right, left, and jump. We will also want to store a reward for reaching checkpoints or finishing the level. \n",
    "\n",
    "To work effectively with our game frames, we will need to convert images, resize, and normalize them. We will also need to use frame stacking to account for motion. We will also want to modify rewards to encourage exploration as the game goes on.\n",
    "\n",
    "Links/References:\n",
    "- Stable Retro GitHub: https://github.com/facebookresearch/stable-retro\n",
    "- OpenAI Gym Retro: https://github.com/openai/retro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution.\n",
    "\n",
    "The goal of our project is to train an AI agent to play Super Mario effectively. To do this, we will use reinforcement learning strategies. The first step for our proposed solution is to define the problem as a Markov Decision Process.\n",
    "\n",
    "The process includes: \n",
    "- The state space of our pixel frames\n",
    "- The action spaces that we will use\n",
    "- The rewards we will use based on progress\n",
    "\n",
    "We would like to start by using q learning techniques. To be specific, we would like to use a Deep Q-Network. This approach means we will use a Convolutional Neural Network to approximate q values for each action. We will also use experience replay and epsilon-greedy exploration for a better training model. \n",
    "\n",
    "The second solution we would like to explore and compare to the first is the Monte Carlo method. Monte Carlo will use episodic learning in our simulation. Given the nature of Mario games, this will be an effective way to use Monte Carlo techniques. We will simulate the episode and then calculate a reward. After that, we will update the q value using the average reward from that certain state-action pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The first evaluation metric we will use is the episode/level completion rate. This is defined as the number of successful completions over the number of attempts. This metric can be applied to every game that we are considering. In Super Mario Bros, the agent may complete a level or it may lose all of its lives or run out of time before completion and would therefore fail. In Street Fighter, the agent may lose the fight which would be considered a fail. We should see the completion rate approach one as the number of trials increases towards infinity. The benchmark for this metric could be the average completion rate that we achieve on the same level/episode.\n",
    "\n",
    "Another possible evaluation metric we could use is the time to complete an episode/level after training the model. Again, lthough we don't have the exact game figured out yet, the time to complete an episode can be applied to nearly all games. For example, in Super Mario Bros, this would be the time to complete a certain level. Time starts when the player/bot is spawned in and ends once it reaches the flagpole. Another possible game could be Street Fighter. The time to complete an episode would be the time it takes to defeat an opponent at a certain difficulty. This metric should decrease as the number of trials increase. The benchmark for this metric could either be the time that an early version of the model itself gets or the time that a human (one of us) gets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) agents optimize based on reward structures, such as greedy action, where short-term gains are prioritized. This could lead to behaviors where the RL agent is exploiting loopholes to maximize its reward in ways that deviate from the intended task. For instance, the agent might prioritize repeatedly collecting coins rather than completing levels efficiently. To counter this, we will introduce randomness through eploration-exploitation strategies, such as epsilon-greedy regularization, to ensure that there is a balance between optimizing known rewards and exploring new behaviors that are outside the local optimum. This will promote robust, adaptable, and ethical decision-making.\n",
    "\n",
    "We acknowledge that the RL agent is trained in a simulated environment that is a representation of the real world. Due to the inherent differences between simulation and reality, there may be unintended consequences of how the model will perform in the real-world based given its learned behavior in the simulation. To mitigate this issue, we will apply safety constraints in reward functions and continuously evaluate the agent's behavior.\n",
    "\n",
    "Reproducubulity is an obstacle in reinforcement learning, as minor changes in hyperparameters can significantly alter performance. To promote ethical AI practices, we will document training methodologies, hyperparameters, and evaluation metrics to enable reproducibility and ensure transparency.\n",
    "\n",
    "Since we are using Stable-Retro to emulated Super Mario Bros, we will ensure compliance with relevant licensing agreements and avoid unauthorized use of proprietary software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use discord/iMessage for out primary method of communication. Discord will be for documents so it is easier to search through, and general conversations will happen on iMessage\n",
    "- In the event of a conflict, the parties involved will try to navigate through the issues themselves. If it persists, the group will have a meeting to discuss any issues, conflicts, and concerns regarding the group. As a last case scenario, we will contact the TA or the professor.\n",
    "- We expect team members to help one another with issues, and collaborate on tasks that members may struggle with\n",
    "- We expect team members to communicate in a respectful and timely manner\n",
    "- We expect team members to work diligently and respect the efforts and time of other members.\n",
    "- We expect team members to hold their work to a high standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/9  |  2 PM |  Brainstorm topics/questions (all)  | Decide on project topic; split up proposal work | \n",
    "| 2/14  |  9 PM |  Complete different parts of proposal | Tweak each other's parts; turn in proposal | \n",
    "| 2/21  | 3 PM  | Have game picked out | Discuss methods to solve game; who will be leading which parts of analysis   |\n",
    "| 2/28  | 3 PM  | Finalized method to solve game | Create game environment; discuss writing the algorithm   |\n",
    "| 3/7  | 3 PM  | Have ideas for algorithm | Implement algorithm |\n",
    "| 3/14  | 12 PM  | Have all bugs fixed | Split up analysis/write-up |\n",
    "| 3/19  | Before 11:59 PM  | Complete Write-up | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Huertas\"></a>1.[^](#Huertas): Huertas, Zambrano, and Diaz Salamanca. Deep Reinforcement Learning for Optimal Gameplay in Street Fighter III: A Resource-Constrained Approach, repositorio.uniandes.edu.co/entities/publication/cb08b574-f75e-4d50-9c43-469099ec6795. <br> \n",
    "<a name=\"Kalose\"></a>2.[^](#Kalose): Kalose, Akshay, et al. “Optimal Battle Strategy in Pokémon Using Reinforcement ...” Optimal Battle Strategy in Pokemon Using Reinforcement Learning , web.stanford.edu/class/aa228/reports/2018/final151.pdf. Accessed 15 Feb. 2025.  <br> \n",
    "<a name=\"Liao\"></a>3.[^](#Lias): Liao, Yizheng, et al. CS229 Final Report Reinforcement Learning to Play Mario, cs229.stanford.edu/proj2012/LiaoYiYang-RLtoPlayMario.pdf. Accessed 15 Feb. 2025.  <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
