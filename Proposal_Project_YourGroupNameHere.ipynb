{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  If you want to do (2) you should fill out the _other_ proposal for that. This is the proposal description for (1).\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your AI system. Generally RL tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible. \n",
    "- You will evaluate the performance of your AI system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Jacob Hanshaw\n",
    "- Jonathan Ito\n",
    "- Hiroki Ito\n",
    "- Rebecca Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorign the gaming world using AI is a subject that has been investigated for years. With the expansion of AI in gaming, there have been numerous uses for AI in gaming, and in other industries. Though classic games such as Super Mario Bros may seem intutive to play, or simple enought with only 6 inputs, there are over 16 different inputs, and different varibales like obstacles, enemies, power up, lives, and timers. By showing that AI is capable of performing so many differnt tasks at once is a great way to show that AI is a reliable method to navigate dynamic enviroemtns. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are numerous methods, but reinforcement learning is by far the most effecient and most explored method. In that regard, there are many instances of AI being used to complete variouos games. In these papers, we see that regardless of the game, pokemon <a name=\"Kalose\"></a>[<sup>[2]</sup>](#Kalose) , mario<a name=\"Liao\"></a>[<sup>[3]</sup>](#Liao), or street fighter <a name=\"Huertas\"></a>[<sup>[1]</sup>](#Huertas), Q-learning and reinforcemnet learning are empahsised. Using these papers as a benchmark on our own agent, we will also utilze similar model adn a similar approach. Though we are taking inspiration from these papers, however, we will be making our own agent and conducting our own tests. We will utilize thier result, however, in regardss to the mario paper, to compare our model with outside resources.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are attempting to utilize reinforcement learning to create a model free AI agent that is capable of completing a given level in Super Mario Bros. In the system, we will utilize Q-learning and measure its learning and success rates. We will also utilize the success of the agent by attempting various other similar and vastly different stages within the game, to measure how quickly it is able to adjust and learn to these new environments. \n",
    "Success will be measured if the number of iterations required to clear a new level is vastly less than the number of attempts required in its initial training in the first level. \n",
    "\n",
    "If time allows us to do so, we will train the initial agent on various levels to test how the learning and adaptability of the agent changes based on the initial level's difficulty and characteristics. (ie. if it is trained on a underwater level, how does it impact the training?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "You should have a strong idea of what dataset(s) will be used to accomplish this project. \n",
    "\n",
    "If you know what (some) of the data you will use, please give the following information for each dataset:\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "\n",
    "If you don't yet know what your dataset(s) will be, you should describe what you desire in terms of the above bullets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The first evaluation metric we will use is the episode/level completion rate. This is defined as the number of successful completions over the number of attempts. This metric can be applied to every game that we are considering. In Super Mario Bros, the agent may complete a level or it may lose all of its lives or run out of time before completion and would therefore fail. In Street Fighter, the agent may lose the fight which would be considered a fail. We should see the completion rate approach one as the number of trials increases towards infinity. The benchmark for this metric could be the average completion rate that we achieve on the same level/episode.\n",
    "\n",
    "Another possible evaluation metric we could use is the time to complete an episode/level after training the model. Again, lthough we don't have the exact game figured out yet, the time to complete an episode can be applied to nearly all games. For example, in Super Mario Bros, this would be the time to complete a certain level. Time starts when the player/bot is spawned in and ends once it reaches the flagpole. Another possible game could be Street Fighter. The time to complete an episode would be the time it takes to defeat an opponent at a certain difficulty. This metric should decrease as the number of trials increase. The benchmark for this metric could either be the time that an early version of the model itself gets or the time that a human (one of us) gets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination. Get creative!\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use discord/iMessage for out primary method of communication. Discord will be for documents so it is easier to search through, and general conversations will happen on iMessage\n",
    "- In the event of a conflict, the parties involved will try to navigate through the issues themselves. If it persists, the group will have a meeting to discuss any issues, conflicts, and concerns regarding the group. As a last case scenario, we will contact the TA or the professor.\n",
    "- We expect team members to help one another with issues, and collaborate on tasks that members may struggle with\n",
    "- We expect team members to communicate in a respectful and timely manner\n",
    "- We expect team members to work diligently and respect the efforts and time of other members.\n",
    "- We expect team members to hold their work to a high standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/9  |  2 PM |  Brainstorm topics/questions (all)  | Decide on project topic; split up proposal work | \n",
    "| 2/14  |  9 PM |  Complete different parts of proposal | Tweak each other's parts; turn in proposal | \n",
    "| 2/21  | 3 PM  | Have game picked out | Discuss methods to solve game; who will be leading which parts of analysis   |\n",
    "| 2/28  | 3 PM  | Finalized method to solve game | Create game environment; discuss writing the algorithm   |\n",
    "| 3/7  | 3 PM  | Have ideas for algorithm | Implement algorithm |\n",
    "| 3/14  | 12 PM  | Have all bugs fixed | Split up analysis/write-up |\n",
    "| 3/19  | Before 11:59 PM  | Complete Write-up | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Huertas\"></a>1.[^](#Huertas): Huertas, Zambrano, and Diaz Salamanca. Deep Reinforcement Learning for Optimal Gameplay in Street Fighter III: A Resource-Constrained Approach, repositorio.uniandes.edu.co/entities/publication/cb08b574-f75e-4d50-9c43-469099ec6795. <br> \n",
    "<a name=\"Kalose\"></a>2.[^](#Kalose): Kalose, Akshay, et al. “Optimal Battle Strategy in Pokémon Using Reinforcement ...” Optimal Battle Strategy in Pokemon Using Reinforcement Learning , web.stanford.edu/class/aa228/reports/2018/final151.pdf. Accessed 15 Feb. 2025.  <br> \n",
    "<a name=\"Liao\"></a>3.[^](#Lias): Liao, Yizheng, et al. CS229 Final Report Reinforcement Learning to Play Mario, cs229.stanford.edu/proj2012/LiaoYiYang-RLtoPlayMario.pdf. Accessed 15 Feb. 2025.  <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
